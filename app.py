# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1awqulMtguKztn_ZoN-O4y8_UMufHfl4Z
"""

!pip install langchain langchain-text-splitters langchain-community
!pip install -U "langchain[google-genai]"
!pip install python-dotenv
!pip install -qU langchain-chroma
!pip install -qU langchain-huggingface
import os
from dotenv import load_dotenv
from langchain.chat_models import init_chat_model
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from datasets import load_dataset
from langchain_community.embeddings import SentenceTransformerEmbeddings
from sentence_transformers import CrossEncoder
from langchain_core.tools import tool
import textwrap
from langchain.agents import create_agent
from sentence_transformers import SentenceTransformer, util
import gradio as gr

pip freeze | grep -E "langchain|gradio|dotenv|sentence-transformers|datasets"

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

# Ortam deÄŸiÅŸkeninden anahtarÄ± al
api_key = os.getenv("GOOGLE_API_KEY")

if not api_key:
    raise ValueError("API key bulunamadÄ±! LÃ¼tfen .env dosyasÄ±nÄ± kontrol et.")

# AnahtarÄ± ortam deÄŸiÅŸkeni olarak ayarla (LangChain bunu bekliyor)
os.environ["GOOGLE_API_KEY"] = api_key

# Modeli baÅŸlat
llm = init_chat_model("google_genai:gemini-2.0-flash")

print("Model baÅŸarÄ±yla yÃ¼klendi ")

model = SentenceTransformer("intfloat/multilingual-e5-large")

sentences = [
    "The weather is lovely today.",
    "It's so sunny outside!",
    "He drove to the stadium."
]
embeddings = model.encode(sentences)

similarities = model.similarity(embeddings, embeddings)
print(similarities.shape)
# [3, 3]

vector_store = Chroma(
    collection_name="example_collection", # AynÄ± veritabanÄ±nda farklÄ± veri gruplarÄ±nÄ± ayÄ±rmak iÃ§in kullanÄ±lÄ±r.
    embedding_function=embeddings,
    persist_directory="./chroma_langchain_db",
)

# Dataset yÃ¼kle
dataset = load_dataset("umarigan/turkiye_finance_qa", split="train")

# Her kaydÄ± Document objesine Ã§evir
docs = [
    Document(
        page_content=record["cevap"],
        metadata={"soru": record["soru"]}
    )
    for record in dataset
]

# Text splitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    add_start_index=True
)

all_splits = text_splitter.split_documents(docs)

print(f"Split blog post into {len(all_splits)} sub-documents.")

# Embedding fonksiyonunu tanÄ±mlayalÄ±m
embedding_function = SentenceTransformerEmbeddings(model_name="intfloat/multilingual-e5-large")

# vector store oluÅŸturuyoruz
vector_store = Chroma.from_documents(
    documents=all_splits,
    embedding=embedding_function,
    persist_directory="./chroma_db" # Vector storeu kaydetmek iÃ§in bir dizin belirleyelim.
)

print("Vector store created and documents added.")

# --- Re-ranker ---
reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
def rerank_results(query, docs):
    scores = reranker.predict([(query, doc.page_content) for doc in docs])
    ranked_docs = [doc for _, doc in sorted(zip(scores, docs), reverse=True)]
    return ranked_docs[:2]  # en alakalÄ± 2 belge

@tool(response_format="content_and_artifact")
def retrieve_context(query: str):
    """
    KullanÄ±cÄ±nÄ±n sorgusuna uygun belgeleri getirir.
    - Vector store'dan en benzer sonuÃ§larÄ± Ã§eker.
    - Skor sÄ±rasÄ±na gÃ¶re sÄ±ralar (en alakalÄ± en Ã¼stte).
    - HatalarÄ± gÃ¼venli biÃ§imde yakalar.
    - Hem biÃ§imlendirilmiÅŸ metin (content), hem ham belgeler (artifact) dÃ¶ndÃ¼rÃ¼r.
    """

    try:
        # En benzer 3 dokÃ¼manÄ± skorlarÄ±yla getir
        retrieved = vector_store.similarity_search_with_score(query, k=3)

        if not retrieved:
            return (
                "Uygun bilgi bulunamadÄ±. Genel bilgiyle yanÄ±t Ã¼retilebilir.",
                []
            )

        formatted_docs = []
        for i, (doc, score) in enumerate(retrieved, start=1):
            source = doc.metadata.get("source", "Bilinmeyen kaynak")
            content_preview = textwrap.fill(doc.page_content.strip(), width=100)
            formatted_docs.append(
                f"ğŸ”¹ **Belge {i} (Benzerlik: {score:.3f})**\n"
                f"ğŸ“˜ Kaynak: {source}\n"
                f"ğŸ“„ Ä°Ã§erik (Ã¶zet):\n{content_preview}\n"
            )

        # TÃ¼m sonuÃ§larÄ± birleÅŸtir
        serialized_output = "\n\n".join(formatted_docs)

        # DÃ¶nÃ¼ÅŸ: (content, artifact)
        return serialized_output, [doc for doc, _ in retrieved]

    except Exception as e:
        print(f"[retrieve_context] Hata: {e}")
        return f"retrieve_context hatasÄ±: {e}", []

# RAG yapÄ±sÄ±nda â€œcevap veren modelâ€ ile â€œveri getiren araÃ§â€ arasÄ±nda kÃ¶prÃ¼
tools = [retrieve_context]
prompt = ("""
    Sen finans alanÄ±nda uzman, Ã§ok dilli bir yapay zekÃ¢ asistansÄ±n.
    KullanÄ±cÄ±larÄ±n finans sistemi, bankacÄ±lÄ±k, yatÄ±rÄ±m araÃ§larÄ±, hisse senetleri, tahviller, dÃ¶viz, emtialar,
    kripto varlÄ±klar, para politikasÄ±, risk yÃ¶netimi ve makroekonomik gÃ¶stergeler hakkÄ±nda sorduklarÄ± sorulara
    doÄŸru, sade ve Ã¶ÄŸretici yanÄ±tlar verirsin.
    Senin bilgi tabanÄ±na eriÅŸim aracÄ±n 'retrieve_context' adÄ±nÄ± taÅŸÄ±r.
    Bu araÃ§, intfloat/multilingual-e5-large modelinden Ã¼retilen vektÃ¶r temsilleri kullanarak ilgili finansal bilgileri getirir.
    YanÄ±t verirken aÅŸaÄŸÄ±daki kurallara dikkat et:
    1. **DoÄŸruluk ve tarafsÄ±zlÄ±k** esastÄ±r; yatÄ±rÄ±m tavsiyesi verme.
    2. **Teknik terimleri aÃ§Ä±kla** ve gerekirse kÄ±sa Ã¶rneklerle sadeleÅŸtir.
    3. **retrieve_context** aracÄ±ndan elde edilen bilgiyi birleÅŸtirerek yanÄ±t oluÅŸtur.
    4. KullanÄ±cÄ± hangi dilde sorarsa o dilde yanÄ±t ver.
    5. EÄŸer baÄŸlam eksikse, finansal prensiplere dayalÄ± genel bilgi ver ama belirsizlik olduÄŸunu belirt.
    YanÄ±t yapÄ±sÄ±:
    - KÄ±sa Ã¶zet
    - Gerekirse detaylÄ± aÃ§Ä±klama veya Ã¶rnek
    - 1-2 cÃ¼mlelik sade bir Ã¶zetle bitir
    KullanÄ±cÄ±nÄ±n amacÄ±: finansal kavramlarÄ± Ã¶ÄŸrenmek, piyasa iÅŸleyiÅŸini anlamak ve temel finansal farkÄ±ndalÄ±k kazanmaktÄ±r.
    retrieve_context aracÄ±yla getirdiÄŸin bilgiyi cevabÄ±na dahil et ve kullanÄ±cÄ±ya doÄŸal bir ÅŸekilde aÃ§Ä±kla.
""")
agent = create_agent(llm, tools)

query = (
   "Faiz oranlarÄ±nÄ±n yÃ¼kselmesi hisse senedi piyasalarÄ±nÄ± nasÄ±l etkiler? "
    "Bu tÃ¼r dÃ¶nemlerde yatÄ±rÄ±mcÄ±lar genellikle nasÄ±l davranÄ±r ve neden?"
)

for event in agent.stream(
    {"messages": [{"role": "user", "content": query}]},
    stream_mode="values",
):
    event["messages"][-1].pretty_print()

results = vector_store.similarity_search("Task Decomposition", k=3)
for r in results:
    print(r.page_content[:300])

# --- Gerekli kÃ¼tÃ¼phaneler ---

# --- Embedding modeli (anlamsal benzerlik iÃ§in) ---
embed_model = SentenceTransformer("intfloat/multilingual-e5-large")

# --- Test seti (kÃ¼Ã§Ã¼k Ã¶rnek QA seti) ---
# Her soruya karÅŸÄ±lÄ±k gelen doÄŸru cevabÄ± ground truth olarak ekliyoruz
test_set = [
    {
        "soru": "Faiz oranlarÄ±nÄ±n artmasÄ± hisse senedi piyasalarÄ±nÄ± nasÄ±l etkiler?",
        "cevap": "Faiz artÄ±ÅŸÄ± genellikle hisse senedi fiyatlarÄ±nÄ± dÃ¼ÅŸÃ¼rÃ¼r Ã§Ã¼nkÃ¼ borÃ§lanma maliyeti artar ve yatÄ±rÄ±mcÄ±lar daha gÃ¼venli araÃ§lara yÃ¶nelir."
    },
    {
        "soru": "Tahvil fiyatlarÄ± faiz oranlarÄ±ndan nasÄ±l etkilenir?",
        "cevap": "Faiz oranlarÄ± yÃ¼kseldiÄŸinde tahvil fiyatlarÄ± dÃ¼ÅŸer, Ã§Ã¼nkÃ¼ yeni tahviller daha yÃ¼ksek getiri sunar ve eski tahviller gÃ¶rece daha az cazip olur."
    }
]

# --- Model cevaplarÄ±nÄ± simÃ¼le et (veya agent ile Ã¼ret) ---
# Ã–rneÄŸin, agent.query(soru) ile gerÃ§ek modelden cevap alÄ±nabilir
model_answers = [
    "Faiz artÄ±ÅŸÄ± hisse fiyatlarÄ±nÄ± genellikle dÃ¼ÅŸÃ¼rÃ¼r, yatÄ±rÄ±mcÄ±lar gÃ¼venli varlÄ±klara yÃ¶nelir.",
    "YÃ¼kselen faiz oranlarÄ± tahvil fiyatlarÄ±nÄ± dÃ¼ÅŸÃ¼rÃ¼r Ã§Ã¼nkÃ¼ yeni tahviller daha yÃ¼ksek getiri saÄŸlar."
]

# --- Benzerlik skoru ve doÄŸruluk Ã¶lÃ§Ã¼mÃ¼ ---
def evaluate_answer(gt_text, model_text):
    emb_gt = embed_model.encode(gt_text)
    emb_model = embed_model.encode(model_text)
    sim_score = util.cos_sim(emb_gt, emb_model).item()
    return sim_score  # 0-1 arasÄ±nda, 1 tam eÅŸleÅŸme

print("=== Otomatik DoÄŸruluk DeÄŸerlendirmesi ===")
for i, qa in enumerate(test_set):
    score = evaluate_answer(qa["cevap"], model_answers[i])
    print(f"Soru: {qa['soru']}")
    print(f"Model CevabÄ±: {model_answers[i]}")
    print(f"Ground Truth: {qa['cevap']}")
    print(f"Benzerlik Skoru: {score:.3f}")
    print("-" * 50)

# --- TutarlÄ±lÄ±k Testi ---
# AynÄ± soruyu farklÄ± ÅŸekilde sorup cevaplar arasÄ±ndaki benzerliÄŸi Ã¶lÃ§
question_variants = [
    "Faizler yÃ¼kseldiÄŸinde borsa ne olur?",
    "Faiz artÄ±ÅŸÄ± hisse senetlerini nasÄ±l etkiler?"
]

# Ã–rnek model cevaplarÄ± (simÃ¼le)
variant_answers = [
    "Faiz artÄ±ÅŸÄ± hisse fiyatlarÄ±nÄ± dÃ¼ÅŸÃ¼rÃ¼r, yatÄ±rÄ±mcÄ±lar gÃ¼venli varlÄ±klara yÃ¶nelir.",
    "Faizlerin yÃ¼kselmesi borsa fiyatlarÄ±nÄ± genellikle aÅŸaÄŸÄ± Ã§eker, riskli varlÄ±klardan uzaklaÅŸÄ±lÄ±r."
]

print("\n=== TutarlÄ±lÄ±k KontrolÃ¼ ===")
sim_score = evaluate_answer(variant_answers[0], variant_answers[1])
print(f"Variant 1: {variant_answers[0]}")
print(f"Variant 2: {variant_answers[1]}")
print(f"TutarlÄ±lÄ±k Skoru (1=tam tutarlÄ±): {sim_score:.3f}")

# --- Global geÃ§miÅŸ listesi (baÄŸlam korunacak) ---
chat_history = []  # LangChain agent'Ä± iÃ§in
display_history = []  # Gradio iÃ§in gÃ¶rÃ¼ntÃ¼lenen geÃ§miÅŸ

def chatbot(user_message, history):
    global chat_history, display_history

    if history is None:
        history = []

    # KullanÄ±cÄ± mesajÄ±nÄ± hem Gradio hem agent geÃ§miÅŸine ekle
    chat_history.append({"role": "user", "content": user_message})

    try:
        # TÃ¼m geÃ§miÅŸi agent'e gÃ¶nder (baÄŸlam korunur)
        response = agent.invoke({"messages": chat_history})

        # Son cevabÄ± al
        answer = response["messages"][-1].content

        # Agent cevabÄ±nÄ± geÃ§miÅŸe ekle
        chat_history.append({"role": "assistant", "content": answer})

        # Debug log (isteÄŸe baÄŸlÄ±)
        print("DEBUG - agent raw response:", repr(response))

    except Exception as e:
        print("DEBUG - agent hata:", e)
        answer = f"Hata: {e}"

    # GÃ¶rsel (Gradio) geÃ§miÅŸi gÃ¼ncelle
    new_history = history + [[user_message, answer]]
    display_history = new_history

    # 2 deÄŸer dÃ¶ndÃ¼r â†’ 1. chatbox iÃ§eriÄŸi, 2. textbox boÅŸ olsun
    return new_history, ""

# --- Gradio ArayÃ¼zÃ¼ ---
with gr.Blocks() as demo:
    gr.Markdown("## ğŸ’¹ Finans AsistanÄ± ")

    chatbox = gr.Chatbot(type="tuples", label="Sohbet")
    user_input = gr.Textbox(placeholder="Sorunuzu yazÄ±n...", label="Soru")
    submit_btn = gr.Button("GÃ¶nder")

    # Enter (submit) ile gÃ¶nderme + kutuyu temizleme
    user_input.submit(
        fn=chatbot,
        inputs=[user_input, chatbox],
        outputs=[chatbox, user_input],
    )

    # Buton ile gÃ¶nderme + kutuyu temizleme
    submit_btn.click(
        fn=chatbot,
        inputs=[user_input, chatbox],
        outputs=[chatbox, user_input],
    )

demo.launch(share=True)
