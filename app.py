# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1awqulMtguKztn_ZoN-O4y8_UMufHfl4Z
"""

!pip install langchain langchain-text-splitters langchain-community
!pip install -U "langchain[google-genai]"
!pip install python-dotenv
!pip install -qU langchain-chroma
!pip install -qU langchain-huggingface
import os
from dotenv import load_dotenv
from langchain.chat_models import init_chat_model
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from datasets import load_dataset
from langchain_community.embeddings import SentenceTransformerEmbeddings
from sentence_transformers import CrossEncoder
from langchain_core.tools import tool
import textwrap
from langchain.agents import create_agent
from sentence_transformers import SentenceTransformer, util
import gradio as gr

pip freeze | grep -E "langchain|gradio|dotenv|sentence-transformers|datasets"

# .env dosyasını yükle
load_dotenv()

# Ortam değişkeninden anahtarı al
api_key = os.getenv("GOOGLE_API_KEY")

if not api_key:
    raise ValueError("API key bulunamadı! Lütfen .env dosyasını kontrol et.")

# Anahtarı ortam değişkeni olarak ayarla (LangChain bunu bekliyor)
os.environ["GOOGLE_API_KEY"] = api_key

# Modeli başlat
llm = init_chat_model("google_genai:gemini-2.0-flash")

print("Model başarıyla yüklendi ")

model = SentenceTransformer("intfloat/multilingual-e5-large")

sentences = [
    "The weather is lovely today.",
    "It's so sunny outside!",
    "He drove to the stadium."
]
embeddings = model.encode(sentences)

similarities = model.similarity(embeddings, embeddings)
print(similarities.shape)
# [3, 3]

vector_store = Chroma(
    collection_name="example_collection", # Aynı veritabanında farklı veri gruplarını ayırmak için kullanılır.
    embedding_function=embeddings,
    persist_directory="./chroma_langchain_db",
)

# Dataset yükle
dataset = load_dataset("umarigan/turkiye_finance_qa", split="train")

# Her kaydı Document objesine çevir
docs = [
    Document(
        page_content=record["cevap"],
        metadata={"soru": record["soru"]}
    )
    for record in dataset
]

# Text splitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    add_start_index=True
)

all_splits = text_splitter.split_documents(docs)

print(f"Split blog post into {len(all_splits)} sub-documents.")

# Embedding fonksiyonunu tanımlayalım
embedding_function = SentenceTransformerEmbeddings(model_name="intfloat/multilingual-e5-large")

# vector store oluşturuyoruz
vector_store = Chroma.from_documents(
    documents=all_splits,
    embedding=embedding_function,
    persist_directory="./chroma_db" # Vector storeu kaydetmek için bir dizin belirleyelim.
)

print("Vector store created and documents added.")

# --- Re-ranker ---
reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
def rerank_results(query, docs):
    scores = reranker.predict([(query, doc.page_content) for doc in docs])
    ranked_docs = [doc for _, doc in sorted(zip(scores, docs), reverse=True)]
    return ranked_docs[:2]  # en alakalı 2 belge

@tool(response_format="content_and_artifact")
def retrieve_context(query: str):
    """
    Kullanıcının sorgusuna uygun belgeleri getirir.
    - Vector store'dan en benzer sonuçları çeker.
    - Skor sırasına göre sıralar (en alakalı en üstte).
    - Hataları güvenli biçimde yakalar.
    - Hem biçimlendirilmiş metin (content), hem ham belgeler (artifact) döndürür.
    """

    try:
        # En benzer 3 dokümanı skorlarıyla getir
        retrieved = vector_store.similarity_search_with_score(query, k=3)

        if not retrieved:
            return (
                "Uygun bilgi bulunamadı. Genel bilgiyle yanıt üretilebilir.",
                []
            )

        formatted_docs = []
        for i, (doc, score) in enumerate(retrieved, start=1):
            source = doc.metadata.get("source", "Bilinmeyen kaynak")
            content_preview = textwrap.fill(doc.page_content.strip(), width=100)
            formatted_docs.append(
                f"🔹 **Belge {i} (Benzerlik: {score:.3f})**\n"
                f"📘 Kaynak: {source}\n"
                f"📄 İçerik (özet):\n{content_preview}\n"
            )

        # Tüm sonuçları birleştir
        serialized_output = "\n\n".join(formatted_docs)

        # Dönüş: (content, artifact)
        return serialized_output, [doc for doc, _ in retrieved]

    except Exception as e:
        print(f"[retrieve_context] Hata: {e}")
        return f"retrieve_context hatası: {e}", []

# RAG yapısında “cevap veren model” ile “veri getiren araç” arasında köprü
tools = [retrieve_context]
prompt = ("""
    Sen finans alanında uzman, çok dilli bir yapay zekâ asistansın.
    Kullanıcıların finans sistemi, bankacılık, yatırım araçları, hisse senetleri, tahviller, döviz, emtialar,
    kripto varlıklar, para politikası, risk yönetimi ve makroekonomik göstergeler hakkında sordukları sorulara
    doğru, sade ve öğretici yanıtlar verirsin.
    Senin bilgi tabanına erişim aracın 'retrieve_context' adını taşır.
    Bu araç, intfloat/multilingual-e5-large modelinden üretilen vektör temsilleri kullanarak ilgili finansal bilgileri getirir.
    Yanıt verirken aşağıdaki kurallara dikkat et:
    1. **Doğruluk ve tarafsızlık** esastır; yatırım tavsiyesi verme.
    2. **Teknik terimleri açıkla** ve gerekirse kısa örneklerle sadeleştir.
    3. **retrieve_context** aracından elde edilen bilgiyi birleştirerek yanıt oluştur.
    4. Kullanıcı hangi dilde sorarsa o dilde yanıt ver.
    5. Eğer bağlam eksikse, finansal prensiplere dayalı genel bilgi ver ama belirsizlik olduğunu belirt.
    Yanıt yapısı:
    - Kısa özet
    - Gerekirse detaylı açıklama veya örnek
    - 1-2 cümlelik sade bir özetle bitir
    Kullanıcının amacı: finansal kavramları öğrenmek, piyasa işleyişini anlamak ve temel finansal farkındalık kazanmaktır.
    retrieve_context aracıyla getirdiğin bilgiyi cevabına dahil et ve kullanıcıya doğal bir şekilde açıkla.
""")
agent = create_agent(llm, tools)

query = (
   "Faiz oranlarının yükselmesi hisse senedi piyasalarını nasıl etkiler? "
    "Bu tür dönemlerde yatırımcılar genellikle nasıl davranır ve neden?"
)

for event in agent.stream(
    {"messages": [{"role": "user", "content": query}]},
    stream_mode="values",
):
    event["messages"][-1].pretty_print()

results = vector_store.similarity_search("Task Decomposition", k=3)
for r in results:
    print(r.page_content[:300])

# --- Gerekli kütüphaneler ---

# --- Embedding modeli (anlamsal benzerlik için) ---
embed_model = SentenceTransformer("intfloat/multilingual-e5-large")

# --- Test seti (küçük örnek QA seti) ---
# Her soruya karşılık gelen doğru cevabı ground truth olarak ekliyoruz
test_set = [
    {
        "soru": "Faiz oranlarının artması hisse senedi piyasalarını nasıl etkiler?",
        "cevap": "Faiz artışı genellikle hisse senedi fiyatlarını düşürür çünkü borçlanma maliyeti artar ve yatırımcılar daha güvenli araçlara yönelir."
    },
    {
        "soru": "Tahvil fiyatları faiz oranlarından nasıl etkilenir?",
        "cevap": "Faiz oranları yükseldiğinde tahvil fiyatları düşer, çünkü yeni tahviller daha yüksek getiri sunar ve eski tahviller görece daha az cazip olur."
    }
]

# --- Model cevaplarını simüle et (veya agent ile üret) ---
# Örneğin, agent.query(soru) ile gerçek modelden cevap alınabilir
model_answers = [
    "Faiz artışı hisse fiyatlarını genellikle düşürür, yatırımcılar güvenli varlıklara yönelir.",
    "Yükselen faiz oranları tahvil fiyatlarını düşürür çünkü yeni tahviller daha yüksek getiri sağlar."
]

# --- Benzerlik skoru ve doğruluk ölçümü ---
def evaluate_answer(gt_text, model_text):
    emb_gt = embed_model.encode(gt_text)
    emb_model = embed_model.encode(model_text)
    sim_score = util.cos_sim(emb_gt, emb_model).item()
    return sim_score  # 0-1 arasında, 1 tam eşleşme

print("=== Otomatik Doğruluk Değerlendirmesi ===")
for i, qa in enumerate(test_set):
    score = evaluate_answer(qa["cevap"], model_answers[i])
    print(f"Soru: {qa['soru']}")
    print(f"Model Cevabı: {model_answers[i]}")
    print(f"Ground Truth: {qa['cevap']}")
    print(f"Benzerlik Skoru: {score:.3f}")
    print("-" * 50)

# --- Tutarlılık Testi ---
# Aynı soruyu farklı şekilde sorup cevaplar arasındaki benzerliği ölç
question_variants = [
    "Faizler yükseldiğinde borsa ne olur?",
    "Faiz artışı hisse senetlerini nasıl etkiler?"
]

# Örnek model cevapları (simüle)
variant_answers = [
    "Faiz artışı hisse fiyatlarını düşürür, yatırımcılar güvenli varlıklara yönelir.",
    "Faizlerin yükselmesi borsa fiyatlarını genellikle aşağı çeker, riskli varlıklardan uzaklaşılır."
]

print("\n=== Tutarlılık Kontrolü ===")
sim_score = evaluate_answer(variant_answers[0], variant_answers[1])
print(f"Variant 1: {variant_answers[0]}")
print(f"Variant 2: {variant_answers[1]}")
print(f"Tutarlılık Skoru (1=tam tutarlı): {sim_score:.3f}")

# --- Global geçmiş listesi (bağlam korunacak) ---
chat_history = []  # LangChain agent'ı için
display_history = []  # Gradio için görüntülenen geçmiş

def chatbot(user_message, history):
    global chat_history, display_history

    if history is None:
        history = []

    # Kullanıcı mesajını hem Gradio hem agent geçmişine ekle
    chat_history.append({"role": "user", "content": user_message})

    try:
        # Tüm geçmişi agent'e gönder (bağlam korunur)
        response = agent.invoke({"messages": chat_history})

        # Son cevabı al
        answer = response["messages"][-1].content

        # Agent cevabını geçmişe ekle
        chat_history.append({"role": "assistant", "content": answer})

        # Debug log (isteğe bağlı)
        print("DEBUG - agent raw response:", repr(response))

    except Exception as e:
        print("DEBUG - agent hata:", e)
        answer = f"Hata: {e}"

    # Görsel (Gradio) geçmişi güncelle
    new_history = history + [[user_message, answer]]
    display_history = new_history

    # 2 değer döndür → 1. chatbox içeriği, 2. textbox boş olsun
    return new_history, ""

# --- Gradio Arayüzü ---
with gr.Blocks() as demo:
    gr.Markdown("## 💹 Finans Asistanı ")

    chatbox = gr.Chatbot(type="tuples", label="Sohbet")
    user_input = gr.Textbox(placeholder="Sorunuzu yazın...", label="Soru")
    submit_btn = gr.Button("Gönder")

    # Enter (submit) ile gönderme + kutuyu temizleme
    user_input.submit(
        fn=chatbot,
        inputs=[user_input, chatbox],
        outputs=[chatbox, user_input],
    )

    # Buton ile gönderme + kutuyu temizleme
    submit_btn.click(
        fn=chatbot,
        inputs=[user_input, chatbox],
        outputs=[chatbox, user_input],
    )

demo.launch(share=True)
